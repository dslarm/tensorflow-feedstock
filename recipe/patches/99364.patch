From c59bd3fdce713f3427ddbfe2dc9fbb01490d0bdc Mon Sep 17 00:00:00 2001
From: aravindhbalaji1985 <aravindhbalaji@gmail.com>
Date: Mon, 25 Aug 2025 15:15:14 -0700
Subject: [PATCH] Support for TFv2.20 to compile with CUDA v12.9.1

---
 tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc     |  2 +-
 .../core/kernels/dynamic_partition_op_gpu.cu.cc       |  6 +++++-
 tensorflow/core/kernels/gpu_prim.h                    |  7 +++----
 tensorflow/core/kernels/split_lib_gpu.cu.cc           |  2 +-
 tensorflow/core/kernels/where_op_gpu.cu.h             | 11 +++++++++++
 5 files changed, 21 insertions(+), 7 deletions(-)

diff --git a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc
index 031464dfdd9ca2..b4fe8ae69771ca 100644
--- a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc
+++ b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc
@@ -70,7 +70,7 @@ __global__ void concat_variable_kernel(
   IntType num_inputs = input_ptr_data.size;
 
   // verbose declaration needed due to template
-  GPU_DYNAMIC_SHARED_MEM_DECL(sizeof(T), unsigned char, smem);
+  GPU_DYNAMIC_SHARED_MEM_DECL(8, unsigned char, smem);
   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);
 
   if (useSmem) {
diff --git a/tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc b/tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc
index ae3fe6d140fef8..737e22e5b9fed0 100644
--- a/tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc
+++ b/tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc
@@ -410,7 +410,11 @@ class DynamicPartitionOpGPU : public AsyncOpKernel {
                                             num_partitions_);
 
 #if GOOGLE_CUDA
-    cub::ConstantInputIterator<int32> values_in(1);
+    #if THRUST_VERSION >= 200802
+        thrust::constant_iterator<int32> values_in(1);
+    #else
+        cub::ConstantInputIterator<int32> values_in(1);
+    #endif
 #elif TENSORFLOW_USE_ROCM
     using ConstantInputIterator =
         ::rocprim::constant_iterator<int32, ptrdiff_t>;
diff --git a/tensorflow/core/kernels/gpu_prim.h b/tensorflow/core/kernels/gpu_prim.h
index bcc873bfb03180..37c0c73119c678 100644
--- a/tensorflow/core/kernels/gpu_prim.h
+++ b/tensorflow/core/kernels/gpu_prim.h
@@ -37,8 +37,8 @@ namespace gpuprim = ::cub;
 
 // Required for sorting Eigen::half and bfloat16.
 namespace cub {
-template <>
-__device__ __forceinline__ void ThreadStoreVolatilePtr<Eigen::half>(
+
+__device__ __forceinline__ void ThreadStoreVolatilePtr(
     Eigen::half *ptr, Eigen::half val, Int2Type<true> /*is_primitive*/) {
   *reinterpret_cast<volatile uint16_t *>(ptr) =
       Eigen::numext::bit_cast<uint16_t>(val);
@@ -50,8 +50,7 @@ __device__ __forceinline__ Eigen::half ThreadLoadVolatilePointer(
   return Eigen::numext::bit_cast<Eigen::half>(result);
 }
 
-template <>
-__device__ __forceinline__ void ThreadStoreVolatilePtr<Eigen::bfloat16>(
+__device__ __forceinline__ void ThreadStoreVolatilePtr(
     Eigen::bfloat16 *ptr, Eigen::bfloat16 val,
     Int2Type<true> /*is_primitive*/) {
   *reinterpret_cast<volatile uint16_t *>(ptr) =
diff --git a/tensorflow/core/kernels/split_lib_gpu.cu.cc b/tensorflow/core/kernels/split_lib_gpu.cu.cc
index 90b28292ac0748..e395d4f12f5a33 100644
--- a/tensorflow/core/kernels/split_lib_gpu.cu.cc
+++ b/tensorflow/core/kernels/split_lib_gpu.cu.cc
@@ -120,7 +120,7 @@ __global__ void split_v_kernel(const T* __restrict__ input_ptr,
   int num_outputs = output_ptr_data.size;
 
   // verbose declaration needed due to template
-  GPU_DYNAMIC_SHARED_MEM_DECL(sizeof(T), unsigned char, smem);
+  GPU_DYNAMIC_SHARED_MEM_DECL(2, unsigned char, smem);
   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);
 
   if (useSmem) {
diff --git a/tensorflow/core/kernels/where_op_gpu.cu.h b/tensorflow/core/kernels/where_op_gpu.cu.h
index 5eb03ec60addd8..3de25b7a82c8e1 100644
--- a/tensorflow/core/kernels/where_op_gpu.cu.h
+++ b/tensorflow/core/kernels/where_op_gpu.cu.h
@@ -233,6 +233,17 @@ class WhereOutputIterator {
     return *(ptr_ + (valid ? (NDIM * n) : 0));
   }
 
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE reference operator*() const {
+    // Dereference the current pointer
+    return *ptr_;
+  }
+
+
+  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE self_type operator+(std::ptrdiff_t n) const {
+    return self_type(ptr_ + NDIM * n, max_row_);
+  }
+
  private:
   int64* ptr_;
   const Eigen::DenseIndex max_row_;
